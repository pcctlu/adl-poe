<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title></title>
</head>
<body>
	<pre>


import os
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras import layers, models
import numpy as np
import matplotlib.pyplot as plt


LATENT_DIM = 128
BATCH_SIZE = 256
EPOCHS = 21
LR = 2e-4
BETA_1 = 0.5
SAMPLE_DIR = "samples"
CKPT_DIR = "checkpoints"
os.makedirs(SAMPLE_DIR, exist_ok=True)
os.makedirs(CKPT_DIR, exist_ok=True)

def preprocess(image, label):
    image = tf.cast(image, tf.float32) / 127.5 - 1.0  # [0,255] -> [-1,1]
    image = tf.reshape(image, [28, 28, 1])
    return image

train_ds = (
    tfds.load('mnist', split='train', as_supervised=True)
    .map(lambda x, y: preprocess(x, y), num_parallel_calls=tf.data.AUTOTUNE)
    .shuffle(60000)
    .batch(BATCH_SIZE, drop_remainder=True)
    .prefetch(tf.data.AUTOTUNE)
)




def build_generator():
    model = models.Sequential(name="generator")
    model.add(layers.Input(shape=(LATENT_DIM,)))
    model.add(layers.Dense(7*7*256, use_bias=False))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU(0.2))
    model.add(layers.Reshape((7, 7, 256)))

    # 7x7 -> 14x14
    model.add(layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU(0.2))

    # 14x14 -> 28x28
    model.add(layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU(0.2))

    # Output: 28x28x1 in [-1,1]
    model.add(layers.Conv2D(1, kernel_size=7, padding='same', activation='tanh'))
    return model

def build_discriminator():
    model = models.Sequential(name="discriminator")
    model.add(layers.Input(shape=(28, 28, 1)))
    # 28->14
    model.add(layers.Conv2D(64, kernel_size=4, strides=2, padding='same'))
    model.add(layers.LeakyReLU(0.2))
    model.add(layers.Dropout(0.3))
    # 14->7
    model.add(layers.Conv2D(128, kernel_size=4, strides=2, padding='same'))
    model.add(layers.LeakyReLU(0.2))
    model.add(layers.Dropout(0.3))
    # 7->4
    model.add(layers.Conv2D(256, kernel_size=4, strides=2, padding='same'))
    model.add(layers.LeakyReLU(0.2))
    model.add(layers.Dropout(0.3))
    model.add(layers.Flatten())

    model.add(layers.Dense(1))
    return model

generator = build_generator()
discriminator = build_discriminator()
generator.summary()
discriminator.summary()



bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)
g_opt = tf.keras.optimizers.Adam(LR, beta_1=BETA_1)
d_opt = tf.keras.optimizers.Adam(LR, beta_1=BETA_1)

# Label helpers (with smoothing & small random flip)
def real_labels(bs):
    # smooth to [0.9, 1.0], then occasionally flip ~5%
    labs = tf.ones((bs, 1)) * tf.random.uniform((bs, 1), 0.9, 1.0)
    flips = tf.cast(tf.random.uniform((bs, 1)) < 0.05, tf.float32)
    return labs * (1 - flips) + (1 - labs) * flips

def fake_labels(bs):
    # smooth to [0.0, 0.1], then occasionally flip ~5%
    labs = tf.zeros((bs, 1)) + tf.random.uniform((bs, 1), 0.0, 0.1)
    flips = tf.cast(tf.random.uniform((bs, 1)) < 0.05, tf.float32)
    return labs * (1 - flips) + (1 - labs) * flips

# Fixed noise for monitoring
fixed_noise = tf.random.normal((16, LATENT_DIM))


@tf.function
def train_step(real_imgs):
    bs = tf.shape(real_imgs)[0]


    noise = tf.random.normal((bs, LATENT_DIM))
    fake_imgs = generator(noise, training=True)

    with tf.GradientTape() as d_tape:
        real_logits = discriminator(real_imgs, training=True)
        fake_logits = discriminator(fake_imgs, training=True)

        d_loss_real = bce(real_labels(bs), real_logits)
        d_loss_fake = bce(fake_labels(bs), fake_logits)
        d_loss = d_loss_real + d_loss_fake

    d_grads = d_tape.gradient(d_loss, discriminator.trainable_variables)
    d_opt.apply_gradients(zip(d_grads, discriminator.trainable_variables))

    noise = tf.random.normal((bs, LATENT_DIM))
    with tf.GradientTape() as g_tape:
        gen_imgs = generator(noise, training=True)
        fake_logits_for_g = discriminator(gen_imgs, training=False)
        g_loss = bce(tf.ones_like(fake_logits_for_g), fake_logits_for_g)

    g_grads = g_tape.gradient(g_loss, generator.trainable_variables)
    g_opt.apply_gradients(zip(g_grads, generator.trainable_variables))

    return d_loss, g_loss


d_losses, g_losses = [], []
ckpt = tf.train.Checkpoint(generator=generator, discriminator=discriminator,
                           g_opt=g_opt, d_opt=d_opt)
manager = tf.train.CheckpointManager(ckpt, CKPT_DIR, max_to_keep=3)

def save_sample_grid(step_or_epoch, noise=fixed_noise, fname_prefix="epoch"):
    imgs = generator(noise, training=False)
    imgs = (imgs + 1.0) * 127.5  # back to [0,255]
    imgs = tf.cast(tf.clip_by_value(imgs, 0, 255), tf.uint8).numpy()

    plt.figure(figsize=(6,6))
    for i in range(16):
        plt.subplot(4,4,i+1)
        plt.imshow(imgs[i, :, :, 0], cmap='gray')
        plt.axis('off')
    plt.suptitle(f"Samples @ {fname_prefix} {step_or_epoch}")
    out_path = os.path.join(SAMPLE_DIR, f"samples_{fname_prefix}_{step_or_epoch:03d}.png")
    plt.savefig(out_path, bbox_inches='tight'); plt.close()

for epoch in range(1, EPOCHS+1):
    d_epoch, g_epoch = [], []
    for real_batch in train_ds:
        d_loss, g_loss = train_step(real_batch)
        d_epoch.append(d_loss.numpy())
        g_epoch.append(g_loss.numpy())

    d_mean, g_mean = float(np.mean(d_epoch)), float(np.mean(g_epoch))
    d_losses.append(d_mean); g_losses.append(g_mean)
    print(f"Epoch {epoch:02d}/{EPOCHS}  D: {d_mean:.4f}  G: {g_mean:.4f}")

    save_sample_grid(epoch, fname_prefix="epoch")
    manager.save()


noise = tf.random.normal((16, LATENT_DIM))
gen_final = generator(noise, training=False)
gen_final = (gen_final + 1.0) * 127.5
gen_final = tf.cast(tf.clip_by_value(gen_final, 0, 255), tf.uint8).numpy()

plt.figure(figsize=(6,6))
for i in range(16):
    plt.subplot(4,4,i+1)
    plt.imshow(gen_final[i,:,:,0], cmap='gray')
    plt.axis('off')
plt.suptitle("Final Samples")
plt.show()

	</pre>

</body>
</html>